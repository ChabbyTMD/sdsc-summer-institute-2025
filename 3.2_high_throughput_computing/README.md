### SDSC Summer Institute 2025
# Session 3.5 High Throughput Computing

**Date:** Tuesday, August 5th, 2025

**Summary**: High-throughput computing (HTC) workloads are characterized by large numbers of small jobs. These frequently involve parameter sweeps where the same type of calculation is done repeatedly with different input values or data processing pipelines where an identical set of operations is applied to many files. This session covers the characteristics and potential pitfalls of HTC, job bundling, the Open Science Grid and the resources available through the Partnership to Advance Throughput Computing (PATh).

**Presented by:** [Marty Kandes](https://www.linkedin.com/in/marty-kandes-b53a34144) (mkandes @sdsc.edu)

### Reading and Presentations:
* **Lecture material:**
   * [COMPLECS: Batch Computing 3: High-Throughput and Many-Task Computing: Slurm Edition](https://drive.google.com/file/d/1fAI7nrBXy1DQgUVs-IoW-Urpg5iuRnXg/view?usp=drive_link)
  
* **Source Code/Examples:**
   * [Batch job arrays](ARRAYS.md)
   * [Batch job dependencies](DEPENDENCIES.md)
   * [Batch job bundling](BUNDLING.md)
   * [Preemptible batch jobs](PREEMPTIBLE.md)
   * [Distributed high-throughput computing](DHTC.md)


#### Additional References
 - [COMPLECS: Batch Computing 1: Working with the Linux Scheduler](https://drive.google.com/file/d/1WO2u6krJbDWudWUUfbmdX7emmmNkUQmb/view?usp=drive_link)
 - [COMPLECS: Batch Computing 2: Getting Started with Batch Job Scheduling: Slurm Edition](https://drive.google.com/file/d/13oLKh71zYmUz9sI4H7Aj0jU0XuUrcBKZ/view?usp=drive_link)


#

[Marty Kandes](https://github.com/mkandes), Computational & Data Science Research Specialist, HPC User Services Group, SDSC
